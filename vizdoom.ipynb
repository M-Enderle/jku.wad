{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VizDoom tournament"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms\n",
    "\n",
    "from arena import VizdoomMPEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize(x):\n",
    "    # batch dimension for interpolation\n",
    "    if x.ndim < 4:\n",
    "        x = x.unsqueeze(0)\n",
    "    return F.interpolate(x, (128, 128))\n",
    "\n",
    "\n",
    "def minmax(x):\n",
    "    return x / 255.0\n",
    "\n",
    "\n",
    "def to_tensor(x):\n",
    "    if isinstance(x, np.ndarray):\n",
    "        x = torch.from_numpy(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "frame_transform = transforms.Compose([to_tensor, minmax, resize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = VizdoomMPEnv(\n",
    "    num_players=2,\n",
    "    num_bots=0,\n",
    "    episode_timeout=5000,\n",
    "    player_transforms=frame_transform,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space, env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random policy (2 players)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = {k: [] for k in range(env.num_players)}\n",
    "\n",
    "for episode in range(2):\n",
    "    ep_return = {k: 0.0 for k in range(env.num_players)}\n",
    "    ep_step = 0\n",
    "    obs = env.reset()\n",
    "    for i, o in enumerate(obs):\n",
    "        frames[i].append(o)\n",
    "    for step in range(100):\n",
    "        act = env.action_space.sample()\n",
    "        obs, rwd, done, info = env.step(act)\n",
    "        ep_return = {k: ep_return[k] + rwd[i] for i, k in enumerate(ep_return)}\n",
    "        for i, o in enumerate(obs):\n",
    "            frames[i].append(o)\n",
    "        if done:\n",
    "            print(\"ep steps: {}; ep return: {}\".format(ep_step, ep_return))\n",
    "            break\n",
    "        else:\n",
    "            ep_step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "from arena.render import render_episode\n",
    "\n",
    "\n",
    "ani = render_episode(frames)\n",
    "HTML(ani.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q-learning\n",
    "Single agent, against bots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "\n",
    "GAMMA = 0.95\n",
    "EPISODES = 100\n",
    "BATCH_SIZE = 512\n",
    "REPLAY_BUFFER_SIZE = 20000\n",
    "LEARNING_RATE = 1e-4\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.1\n",
    "EPSILON_DECAY = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = VizdoomMPEnv(\n",
    "    num_players=1,\n",
    "    num_bots=2,\n",
    "    episode_timeout=5000,\n",
    "    player_transforms=frame_transform,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, action_space=6):\n",
    "        super().__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=8, stride=4),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(64, 32, kernel_size=3, stride=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.SiLU(),\n",
    "        )\n",
    "\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(32 * 6 * 6, 512), nn.SiLU(), nn.Linear(512, action_space)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "dqn = DQN(action_space=env.action_space.n).to(device)\n",
    "optimizer = optim.Adam(dqn.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(f\"Parameters: {sum(p.numel() for p in dqn.parameters()) / 1e3:.1f}K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replay buffer\n",
    "replay_buffer = deque(maxlen=REPLAY_BUFFER_SIZE)\n",
    "\n",
    "\n",
    "@torch.no_grad\n",
    "def epsilon_greedy(state, epsilon):\n",
    "    if random.random() < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        state = state.to(device)\n",
    "        q_values = dqn(state)\n",
    "        return q_values.argmax().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = EPSILON_START\n",
    "steps_done = 0\n",
    "q_loss_list = []  # Track Q-loss per episode\n",
    "reward_list = []  # Track total rewards per episode\n",
    "\n",
    "for episode in range(EPISODES):\n",
    "    ep_return = 0.0\n",
    "    ep_step = 0\n",
    "    obs = env.reset()\n",
    "    obs = obs[0]  # Single player\n",
    "\n",
    "    for step in range(100):\n",
    "        act = epsilon_greedy(obs, epsilon)\n",
    "        next_obs, rwd, done, info = env.step(act)\n",
    "\n",
    "        # Single player adjustments\n",
    "        rwd = rwd[0]\n",
    "        next_obs = next_obs[0]\n",
    "\n",
    "        # Store in replay buffer\n",
    "        replay_buffer.append((obs, act, rwd, next_obs, done))\n",
    "\n",
    "        obs = next_obs\n",
    "        ep_return += rwd\n",
    "\n",
    "        # Train if buffer has enough samples\n",
    "        if len(replay_buffer) > BATCH_SIZE:\n",
    "            batch = random.sample(replay_buffer, BATCH_SIZE)\n",
    "            states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "            states = torch.cat(states).to(device, dtype=torch.float32)\n",
    "            next_states = torch.cat(next_states, 0).to(device, dtype=torch.float32)\n",
    "            actions = torch.tensor(actions, dtype=torch.long).to(device)\n",
    "            rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "            dones = torch.tensor(dones, dtype=torch.float32).to(device)\n",
    "\n",
    "            q_values = dqn(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                next_q_values = dqn(next_states).max(1).values\n",
    "                target_q_values = rewards + GAMMA * next_q_values * (1 - dones)\n",
    "\n",
    "            loss = F.mse_loss(q_values, target_q_values)\n",
    "            q_loss_list.append(loss.item())  # Store loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Update epsilon\n",
    "        epsilon = max(EPSILON_END, EPSILON_START - steps_done / EPSILON_DECAY)\n",
    "        steps_done += 1\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "        else:\n",
    "            ep_step += 1\n",
    "\n",
    "    reward_list.append(ep_return)\n",
    "\n",
    "    avg_reward = np.mean(reward_list[-10:])\n",
    "    print(f\"Episode {episode + 1}/{EPISODES}, steps: {ep_step}, epsilon: {epsilon:.2f}\")\n",
    "    print(f\"\\tReturn: {ep_return:.2f}, avg Reward (last 10): {avg_reward:.2f}\")\n",
    "    if len(q_loss_list) > 0:\n",
    "        avg_q_loss = np.mean(q_loss_list[-10:])\n",
    "        print(f\"\\tAvg Q-loss: {avg_q_loss:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = {k: [] for k in range(env.num_players)}\n",
    "\n",
    "ep_return = {k: 0.0 for k in range(env.num_players)}\n",
    "ep_step = 0\n",
    "done = False\n",
    "obs = env.reset()\n",
    "for i, o in enumerate(obs):\n",
    "    frames[i].append(o)\n",
    "while not done:\n",
    "    obs = obs[0].to(device)\n",
    "    act = dqn(obs).argmax().item()\n",
    "    obs, rwd, done, info = env.step(act)\n",
    "    ep_return = {k: ep_return[k] + rwd[i] for i, k in enumerate(ep_return)}\n",
    "    for i, o in enumerate(obs):\n",
    "        frames[i].append(o)\n",
    "    if done:\n",
    "        print(\"ep steps: {}; ep return: {}\".format(ep_step, ep_return))\n",
    "        break\n",
    "    else:\n",
    "        ep_step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "from arena.render import render_episode\n",
    "\n",
    "\n",
    "ani = render_episode(frames)\n",
    "HTML(ani.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mhd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
